{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import bs4.builder._lxml\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = 'https://www.mass.gov'\n",
    "start = 'https://www.mass.gov/guides/list-of-court-forms-by-topic'\n",
    "connection = requests.get(start)\n",
    "html = connection.text\n",
    "soup = bs4.BeautifulSoup(html,\"lxml\")\n",
    "# print(soup)\n",
    "connection.close()\n",
    "div = soup.findAll(\"div\", class_ = \"ma__rich-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hreflist = []\n",
    "for category in div:\n",
    "    type_of_form = category.ul\n",
    "    if type_of_form!=None:\n",
    "        for a in type_of_form.findAll(\"a\"):\n",
    "            hreflist.append(a['href'])\n",
    "            \n",
    "                        \n",
    "print(hreflist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.mass.gov/lists/probate-and-family-court-forms-for-wills-estates-and-trusts#accounting-forms-\n"
     ]
    }
   ],
   "source": [
    "formcategory = BASE + hreflist[0]\n",
    "# print(formcategory)\n",
    "connection = requests.get(formcategory)\n",
    "html = connection.text\n",
    "soup = bs4.BeautifulSoup(html,\"lxml\")\n",
    "print(formcategory)\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json \n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import date\n",
    "import os\n",
    "from os import walk\n",
    "import os.path\n",
    "from os import path\n",
    "import uuid\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def hashme(w):\n",
    "    h = hashlib.md5(w.encode('utf-8'))\n",
    "    return h.hexdigest()\n",
    "\n",
    "    import requests\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.parse import quote\n",
    "\n",
    "#!pip install PyPDF2\n",
    "import PyPDF2\n",
    "\n",
    "from io import StringIO # https://stackoverflow.com/a/18284900\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "def convert(fname, pages=None):\n",
    "    if not pages:\n",
    "        pagenums = set()\n",
    "    else:\n",
    "        pagenums = set(pages)\n",
    "\n",
    "    output = StringIO()\n",
    "    manager = PDFResourceManager()\n",
    "    converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "    infile = open(fname, 'rb') # python three change\n",
    "    for page in PDFPage.get_pages(infile, pagenums, check_extractable=False):\n",
    "        interpreter.process_page(page)\n",
    "    infile.close()\n",
    "    converter.close()\n",
    "    text = output.getvalue()\n",
    "    output.close\n",
    "    return text\n",
    "    \n",
    "def download_pdf(url,path,filename):\n",
    "    try:\n",
    "        \n",
    "        filename = path+filename\n",
    "\n",
    "        f = requests.get(url, stream=True, headers={'User-agent': 'Mozilla/5.0'})\n",
    "\n",
    "        with open(filename, \"wb\") as code:\n",
    "            #code.write(f.read())\n",
    "            code.write(f.content)\n",
    "\n",
    "        return convert(filename)\n",
    "    except:\n",
    "        return \"error\"\n",
    "\n",
    "def connect_to_page(url):\n",
    "    connection = requests.get(url)\n",
    "    html = connection.text\n",
    "    soup = bs4.BeautifulSoup(html,\"lxml\")\n",
    "    connection.close()\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf_unmasked(url,path,filename):\n",
    "    \n",
    "        \n",
    "    filename = path+filename\n",
    "\n",
    "    f = requests.get(url, stream=True, headers={'User-agent': 'Mozilla/5.0'})\n",
    "\n",
    "    with open(filename, \"wb\") as code:\n",
    "        #code.write(f.read())\n",
    "        code.write(f.content)\n",
    "\n",
    "    return convert(filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = {}\n",
    "pdfs = {}\n",
    "redirections = []\n",
    "files_df = pd.DataFrame([],columns=[\"id\",\"jurisdiction\",\"source\",\"title\",\"group\",\"url\",\"filename\",\"downloaded\",\"status\"])\n",
    "form_dest = \"PDFS/D/\"\n",
    "today = date.today().strftime(\"%Y-%m-%d\")\n",
    "#remove the parenthesis\n",
    "pattern = r\"\\([^()]*\\)\"\n",
    "source = \"https://www.mass.gov/guides/list-of-court-forms-by-topic\"\n",
    "jur = \"MA\"\n",
    "status = 0\n",
    "for href in range(len(hreflist)):\n",
    "    time.sleep(2)\n",
    "    formcategory = BASE + hreflist[href]\n",
    "    connection = requests.get(formcategory)\n",
    "    html = connection.text\n",
    "    soup = bs4.BeautifulSoup(html,\"lxml\")\n",
    "    print(formcategory)\n",
    "    connection.close()\n",
    "    try:\n",
    "        groups = soup.find(\"div\",class_ = \"ma__stacked-row stacked-row-column\")\n",
    "        section = groups.find_all(\"h2\")\n",
    "        links = soup.find_all(\"div\", class_ = \"ma__form-downloads__links\")\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    for group in range(len(section)-1): #-1 because last is contact\n",
    "            forms = links[group].find_all(class_ = \"ma__download-link__file-link\")\n",
    "            redirects = links[group].find_all(class_ = \"js-clickable-link\")\n",
    "            curr_group = section[group].text\n",
    "            print(curr_group)\n",
    "            \n",
    "            # redirects result into an error download\n",
    "            if redirects!=None:\n",
    "                for i in redirects:\n",
    "                    if \"https\" not in i['href']:\n",
    "                        inner_url = BASE + i['href']\n",
    "                        time.sleep(2)\n",
    "                        inner_soup = connect_to_page(inner_url)\n",
    "                        inner_div = inner_soup.findAll(\"a\", href= True)\n",
    "                        for i in inner_div:\n",
    "                            if \"courtforms\" in i['href']:\n",
    "                                url = i['href']\n",
    "                                title = re.sub(pattern,\"\",i.text)\n",
    "                                filename = url.split(\"//\")[1].split(\"/\")[-1] +\".pdf\"\n",
    "                                \n",
    "                                if filename not in pdfs:\n",
    "                                    print(title)\n",
    "                                    print(filename)\n",
    "                                    pdfs[filename] = title\n",
    "                                    fileid = hashme(url)\n",
    "                                    status = download_pdf(url,form_dest,fileid +\".pdf\")\n",
    "                                    \n",
    "                                    if status=='error':\n",
    "                                        print(status, url)\n",
    "                                        status = 1\n",
    "                                        errors[filename] = title\n",
    "                                    else: status = 0\n",
    "                                    files_df = files_df.append(pd.DataFrame([[fileid,jur,source,title,curr_group,url,filename,today,status]],columns=[\"id\",\"jurisdiction\",\"source\",\"title\",\"group\",\"url\",\"filename\",\"downloaded\",\"status\"]))\n",
    "                                status = 0\n",
    "                    else:\n",
    "                        redirections.append(i['href'])\n",
    "\n",
    "\n",
    "            for f in forms:\n",
    "                url = f['href']\n",
    "                filename = url.split(\"//\")[1].split(\"/\")[-2] +\".pdf\"\n",
    "                \n",
    "                if filename not in pdfs:\n",
    "                    title = re.sub(pattern,\"\",f.text.split(\",\")[2].split(\"\\n\")[1].lstrip())\n",
    "                    pdfs[filename] = title\n",
    "                    fileid = hashme(url)\n",
    "                    #print(curr_group)\n",
    "                    print(title)\n",
    "                    print(filename)\n",
    "                    status = download_pdf(url,form_dest,fileid+\".pdf\")\n",
    "                    \n",
    "                    if status=='error':\n",
    "                        print(status, url)\n",
    "                        status = 1\n",
    "                        errors[filename] = title\n",
    "                    else: status = 0\n",
    "                    files_df = files_df.append(pd.DataFrame([[fileid,jur,source,title,curr_group,url,filename,today,status]],columns=[\"id\",\"jurisdiction\",\"source\",\"title\",\"group\",\"url\",\"filename\",\"downloaded\",\"status\"]))\n",
    "                status = 0\n",
    "if len(files_df) > 0:\n",
    "    files_df.to_csv(\"All.csv\",index=False, encoding=\"utf-8\")\n",
    "\n",
    "# with open('C_errors.pickle', 'wb') as error_file:\n",
    "#     pickle.dump(errors, error_file)\n",
    "# with open('C_pdfs.pickle', 'wb') as pdfs_file:\n",
    "#     pickle.dump(pdfs, pdfs_file)\n",
    "# with open('C_redirections.pickle', 'wb') as inner_sites_file:\n",
    "#     pickle.dump(redirections, inner_sites_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below was written deal with multiple downloading errors that occured. Instead of directly downloading the forms from the url, I used selenium to click a download button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "def download(download_url, filename):\n",
    "    response = urllib.request.urlopen(download_url)\n",
    "    file = open(filename + \".pdf\", 'wb')\n",
    "    file.write(response.read())\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "import webbrowser\n",
    "import io\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"\\([^()]*\\)\"\n",
    "folder = 'C:/Users/Owner/Downloads'\n",
    "file_list = []\n",
    "\n",
    "# a bit of cleaning\n",
    "dest_sample = 'PDFS/'\n",
    "df = pd.read_csv('All.csv')\n",
    "df_errors = df.loc[df['status']==1]\n",
    "url_list = list(df_errors['url'])\n",
    "ids_list = list(df_errors['id'])\n",
    "index_list = df_errors.index\n",
    "\n",
    "print(len(df_errors))\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "\n",
    "hash_list = []\n",
    "for url in range(len(url_list)):\n",
    "    time.sleep(2)\n",
    "    if 'www' not in url_list[url]:\n",
    "        print(url_list[url], url)\n",
    "        driver.get(url_list[url])\n",
    "        \n",
    "        try:\n",
    "            button = driver.find_element_by_css_selector('widget.buttonfieldwidget.xfaButton > input')\n",
    "            print(button)\n",
    "            button.click()\n",
    "            df.at[index_list[url],'status'] = 0\n",
    "            hash_list.append(hashme(url_list[url]))\n",
    "        except:\n",
    "            print('######################')\n",
    "            print('ERROR')\n",
    "            print('######################')\n",
    "df.to_csv(\"All_cleaned2.csv\",index=False, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "\n",
    "# for url in range(len(url_list)):\n",
    "#     if 'download'==url_list[url].split('/')[-1]:\n",
    "#         print(url_list[url], index_list[url])\n",
    "#         status = download_pdf_unmasked(url_list[url],dest_sample,ids_list[url]+'.pdf')\n",
    "#         if status == 'error':\n",
    "#             print('#######################')\n",
    "#             print('ERROR')\n",
    "#             print('#######################')\n",
    "#         else:\n",
    "#             df.at[index_list[url],'status'] = 0\n",
    "\n",
    "# df.to_csv(\"All_cleaned.csv\",index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from selenium.webdriver.common.by import By\n",
    "def hashme(w):\n",
    "    h = hashlib.md5(w.encode('utf-8'))\n",
    "    return h.hexdigest()\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "import webbrowser\n",
    "import io\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dest_sample = 'PDFS/'\n",
    "df = pd.read_csv('All.csv')\n",
    "df_errors = df.loc[df['status']==1]\n",
    "url_list = list(df_errors['url'])\n",
    "ids_list = list(df_errors['id'])\n",
    "index_list = df_errors.index\n",
    "\n",
    "#print(index_list)\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "hash_list = []\n",
    "for url in range(76,len(url_list)):\n",
    "    time.sleep(2)\n",
    "    if 'www' not in url_list[url]:\n",
    "        print(url_list[url], url)\n",
    "        driver.get(url_list[url])\n",
    "        \n",
    "        driver.find_element_by_css_selector(\"[aria-label='Save as PDF'\").click()#this is me most common layout. However, not nearly the only one..\n",
    "        #as a result some where manually downloaded\n",
    "        # df.at[index_list[url],'status'] = 0\n",
    "\n",
    "#20, 31, 33, 37, 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'C:/Users/Owner/Downloads'\n",
    "df = pd.read_csv('All_cleaned2.csv')\n",
    "df_errors = df.loc[df['status']==1]\n",
    "url_list = list(df_errors['url'])\n",
    "ids_list = list(df_errors['id'])\n",
    "index_list = df_errors.index\n",
    "\n",
    "len(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder = 'C:/Users/Owner/Downloads'\n",
    "errors_set = set(list([20, 31, 33, 37, 41])) #these were unsuccesful attempts. 2 were docs and the other 4 needed to be filled out before downloading\n",
    "successful_ids = []\n",
    "\n",
    "for id in range(len(ids_list)):\n",
    "    if id not in errors_set:\n",
    "        successful_ids.append(ids_list[id])\n",
    "    # else:\n",
    "    #     print(ids_list[id])\n",
    "# for i in errors_set:\n",
    "#     print(ids_list[i])\n",
    "\n",
    "# navigate in our downloads directory and rename the files we just downloaded\n",
    "# os.chdir(folder)\n",
    "# files = glob.glob(\"*.pdf\") #but only the pdfs \n",
    "# files.sort(key=os.path.getmtime) # from the first (download) to the last so it is consistent with our database\n",
    "# for id in range(len(successful_ids)):\n",
    "   \n",
    "#     filename = os.fsdecode(files[id])\n",
    "#     print(filename)\n",
    "#     dst = f\"{successful_ids[id]}.pdf\" \n",
    "#     src =f\"{folder}/{filename}\"\n",
    "#     os.rename(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('All.csv')\n",
    "df_errors = df.loc[df['status']==1]\n",
    "url_list = list(df_errors['url'])\n",
    "ids_list = list(df_errors['id'])\n",
    "index_list = df_errors.index\n",
    "\n",
    "# to_change = df[df['id']==successful_ids[6]]\n",
    "\n",
    "for id in range(len(successful_ids)):\n",
    "    to_change = df[df['id']==successful_ids[id]].index\n",
    "    df.at[to_change,'status'] = 0\n",
    "df.to_csv(\"Trial.csv\",index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retval = os.getcwd()\n",
    "print(retval)\n",
    "folder = 'PDFS/G'\n",
    "os.chdir(folder)\n",
    "files = glob.glob(\"*.pdf\")\n",
    "for file in files:\n",
    "    print(file)\n",
    "retval = os.getcwd()\n",
    "os.chdir(retval)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f68585656d2af418509cfdc3ae64a2df251c855d829aa9c64f1046c3e79900"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
