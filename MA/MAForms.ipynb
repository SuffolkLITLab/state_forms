{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import bs4.builder._lxml\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = 'https://www.mass.gov'\n",
    "start = 'https://www.mass.gov/guides/list-of-court-forms-by-topic'\n",
    "connection = requests.get(start)\n",
    "html = connection.text\n",
    "soup = bs4.BeautifulSoup(html,\"lxml\")\n",
    "# print(soup)\n",
    "connection.close()\n",
    "div = soup.findAll(\"div\", class_ = \"ma__rich-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hreflist = []\n",
    "for category in div:\n",
    "    type_of_form = category.ul\n",
    "    if type_of_form!=None:\n",
    "        for a in type_of_form.findAll(\"a\"):\n",
    "            hreflist.append(a['href'])\n",
    "            \n",
    "                        \n",
    "print(hreflist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.mass.gov/lists/probate-and-family-court-forms-for-wills-estates-and-trusts#accounting-forms-\n"
     ]
    }
   ],
   "source": [
    "formcategory = BASE + hreflist[0]\n",
    "# print(formcategory)\n",
    "connection = requests.get(formcategory)\n",
    "html = connection.text\n",
    "soup = bs4.BeautifulSoup(html,\"lxml\")\n",
    "print(formcategory)\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json \n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import date\n",
    "import os\n",
    "from os import walk\n",
    "import os.path\n",
    "from os import path\n",
    "import uuid\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def hashme(w):\n",
    "    h = hashlib.md5(w.encode('utf-8'))\n",
    "    return h.hexdigest()\n",
    "\n",
    "    import requests\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.parse import quote\n",
    "\n",
    "#!pip install PyPDF2\n",
    "import PyPDF2\n",
    "\n",
    "from io import StringIO # https://stackoverflow.com/a/18284900\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "def convert(fname, pages=None):\n",
    "    if not pages:\n",
    "        pagenums = set()\n",
    "    else:\n",
    "        pagenums = set(pages)\n",
    "\n",
    "    output = StringIO()\n",
    "    manager = PDFResourceManager()\n",
    "    converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "    infile = open(fname, 'rb') # python three change\n",
    "    for page in PDFPage.get_pages(infile, pagenums, check_extractable=False):\n",
    "        interpreter.process_page(page)\n",
    "    infile.close()\n",
    "    converter.close()\n",
    "    text = output.getvalue()\n",
    "    output.close\n",
    "    return text\n",
    "    \n",
    "def download_pdf(url,path,filename):\n",
    "    try:\n",
    "        \n",
    "        filename = path+filename\n",
    "\n",
    "        f = requests.get(url, stream=True, headers={'User-agent': 'Mozilla/5.0'})\n",
    "\n",
    "        with open(filename, \"wb\") as code:\n",
    "            #code.write(f.read())\n",
    "            code.write(f.content)\n",
    "\n",
    "        return convert(filename)\n",
    "    except:\n",
    "        return \"error\"\n",
    "\n",
    "def connect_to_page(url):\n",
    "    connection = requests.get(url)\n",
    "    html = connection.text\n",
    "    soup = bs4.BeautifulSoup(html,\"lxml\")\n",
    "    connection.close()\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf_unmasked(url,path,filename):\n",
    "    \n",
    "        \n",
    "    filename = path+filename\n",
    "\n",
    "    f = requests.get(url, stream=True, headers={'User-agent': 'Mozilla/5.0'})\n",
    "\n",
    "    with open(filename, \"wb\") as code:\n",
    "        #code.write(f.read())\n",
    "        code.write(f.content)\n",
    "\n",
    "    return convert(filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = {}\n",
    "pdfs = {}\n",
    "redirections = []\n",
    "files_df = pd.DataFrame([],columns=[\"id\",\"jurisdiction\",\"source\",\"title\",\"group\",\"url\",\"filename\",\"downloaded\",\"status\"])\n",
    "form_dest = \"PDFS/D/\"\n",
    "today = date.today().strftime(\"%Y-%m-%d\")\n",
    "#remove the parenthesis\n",
    "pattern = r\"\\([^()]*\\)\"\n",
    "source = \"https://www.mass.gov/guides/list-of-court-forms-by-topic\"\n",
    "jur = \"MA\"\n",
    "status = 0\n",
    "for href in range(len(hreflist)):\n",
    "    time.sleep(2)\n",
    "    formcategory = BASE + hreflist[href]\n",
    "    connection = requests.get(formcategory)\n",
    "    html = connection.text\n",
    "    soup = bs4.BeautifulSoup(html,\"lxml\")\n",
    "    print(formcategory)\n",
    "    connection.close()\n",
    "    try:\n",
    "        groups = soup.find(\"div\",class_ = \"ma__stacked-row stacked-row-column\")\n",
    "        section = groups.find_all(\"h2\")\n",
    "        links = soup.find_all(\"div\", class_ = \"ma__form-downloads__links\")\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    for group in range(len(section)-1): #-1 because last is contact\n",
    "            forms = links[group].find_all(class_ = \"ma__download-link__file-link\")\n",
    "            redirects = links[group].find_all(class_ = \"js-clickable-link\")\n",
    "            curr_group = section[group].text\n",
    "            print(curr_group)\n",
    "            \n",
    "            # redirects result into an error download\n",
    "            if redirects!=None:\n",
    "                for i in redirects:\n",
    "                    if \"https\" not in i['href']:\n",
    "                        inner_url = BASE + i['href']\n",
    "                        time.sleep(2)\n",
    "                        inner_soup = connect_to_page(inner_url)\n",
    "                        inner_div = inner_soup.findAll(\"a\", href= True)\n",
    "                        for i in inner_div:\n",
    "                            if \"courtforms\" in i['href']:\n",
    "                                url = i['href']\n",
    "                                title = re.sub(pattern,\"\",i.text)\n",
    "                                filename = url.split(\"//\")[1].split(\"/\")[-1] +\".pdf\"\n",
    "                                \n",
    "                                if filename not in pdfs:\n",
    "                                    print(title)\n",
    "                                    print(filename)\n",
    "                                    pdfs[filename] = title\n",
    "                                    fileid = hashme(url)\n",
    "                                    status = download_pdf(url,form_dest,fileid +\".pdf\")\n",
    "                                    \n",
    "                                    if status=='error':\n",
    "                                        print(status, url)\n",
    "                                        status = 1\n",
    "                                        errors[filename] = title\n",
    "                                    else: status = 0\n",
    "                                    files_df = files_df.append(pd.DataFrame([[fileid,jur,source,title,curr_group,url,filename,today,status]],columns=[\"id\",\"jurisdiction\",\"source\",\"title\",\"group\",\"url\",\"filename\",\"downloaded\",\"status\"]))\n",
    "                                status = 0\n",
    "                    else:\n",
    "                        redirections.append(i['href'])\n",
    "\n",
    "\n",
    "            for f in forms:\n",
    "                url = f['href']\n",
    "                filename = url.split(\"//\")[1].split(\"/\")[-2] +\".pdf\"\n",
    "                \n",
    "                if filename not in pdfs:\n",
    "                    title = re.sub(pattern,\"\",f.text.split(\",\")[2].split(\"\\n\")[1].lstrip())\n",
    "                    pdfs[filename] = title\n",
    "                    fileid = hashme(url)\n",
    "                    #print(curr_group)\n",
    "                    print(title)\n",
    "                    print(filename)\n",
    "                    status = download_pdf(url,form_dest,fileid+\".pdf\")\n",
    "                    \n",
    "                    if status=='error':\n",
    "                        print(status, url)\n",
    "                        status = 1\n",
    "                        errors[filename] = title\n",
    "                    else: status = 0\n",
    "                    files_df = files_df.append(pd.DataFrame([[fileid,jur,source,title,curr_group,url,filename,today,status]],columns=[\"id\",\"jurisdiction\",\"source\",\"title\",\"group\",\"url\",\"filename\",\"downloaded\",\"status\"]))\n",
    "                status = 0\n",
    "if len(files_df) > 0:\n",
    "    files_df.to_csv(\"All.csv\",index=False, encoding=\"utf-8\")\n",
    "\n",
    "# with open('C_errors.pickle', 'wb') as error_file:\n",
    "#     pickle.dump(errors, error_file)\n",
    "# with open('C_pdfs.pickle', 'wb') as pdfs_file:\n",
    "#     pickle.dump(pdfs, pdfs_file)\n",
    "# with open('C_redirections.pickle', 'wb') as inner_sites_file:\n",
    "#     pickle.dump(redirections, inner_sites_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below was written deal with multiple downloading errors that occured. Instead of directly downloading the forms from the url, I used selenium to click a download button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "def download(download_url, filename):\n",
    "    response = urllib.request.urlopen(download_url)\n",
    "    file = open(filename + \".pdf\", 'wb')\n",
    "    file.write(response.read())\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "import webbrowser\n",
    "import io\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 97.0.4692\n",
      "Get LATEST chromedriver version for 97.0.4692 google-chrome\n",
      "Driver [C:\\Users\\Owner\\.wdm\\drivers\\chromedriver\\win32\\97.0.4692.71\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "https://courtforms.jud.state.ma.us/publicforms/PFC0042 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_27172\\2090484818.py:23: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  button = driver.find_element_by_css_selector('widget.buttonfieldwidget.xfaButton > input')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################\n",
      "ERROR\n",
      "######################\n",
      "https://courtforms.jud.state.ma.us/publicforms/PFC0037 1\n",
      "######################\n",
      "ERROR\n",
      "######################\n",
      "https://courtforms.jud.state.ma.us/publicforms/PFC0064 2\n",
      "######################\n",
      "ERROR\n",
      "######################\n",
      "https://courtforms.jud.state.ma.us/publicforms/PFC0008 3\n",
      "######################\n",
      "ERROR\n",
      "######################\n",
      "https://courtforms.jud.state.ma.us/publicforms/PFC0029 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# a bit of cleaning\n",
    "dest_sample = 'PDFS/'\n",
    "df = pd.read_csv('All.csv')\n",
    "df_errors = df.loc[df['status']==1]\n",
    "url_list = list(df_errors['url'])\n",
    "ids_list = list(df_errors['id'])\n",
    "index_list = df_errors.index\n",
    "\n",
    "print(len(df_errors))\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "\n",
    "hash_list = []\n",
    "for url in range(len(url_list)):\n",
    "    time.sleep(2)\n",
    "    if 'www' not in url_list[url]:\n",
    "        print(url_list[url], url)\n",
    "        driver.get(url_list[url])\n",
    "        \n",
    "        try:\n",
    "            button = driver.find_element_by_css_selector('widget.buttonfieldwidget.xfaButton > input')\n",
    "            print(button)\n",
    "            button.click()\n",
    "            df.at[index_list[url],'status'] = 0\n",
    "            hash_list.append(hashme(url_list[url]))\n",
    "        except:\n",
    "            print('######################')\n",
    "            print('ERROR')\n",
    "            print('######################')\n",
    "df.to_csv(\"All_cleaned2.csv\",index=False, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "\n",
    "# for url in range(len(url_list)):\n",
    "#     if 'download'==url_list[url].split('/')[-1]:\n",
    "#         print(url_list[url], index_list[url])\n",
    "#         status = download_pdf_unmasked(url_list[url],dest_sample,ids_list[url]+'.pdf')\n",
    "#         if status == 'error':\n",
    "#             print('#######################')\n",
    "#             print('ERROR')\n",
    "#             print('#######################')\n",
    "#         else:\n",
    "#             df.at[index_list[url],'status'] = 0\n",
    "\n",
    "# df.to_csv(\"All_cleaned.csv\",index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"\\([^()]*\\)\"\n",
    "folder = 'C:/Users/Owner/Downloads'\n",
    "file_list = []\n",
    "\n",
    "# paths = sorted(Path(folder).iterdir(), key=os.path.getmtime)\n",
    "# print(paths)\n",
    "# title = re.sub(pattern,\"\",i.text)\n",
    "# for count, filename in enumerate(os.listdir(paths)):\n",
    "#     # file_list.append(filename)\n",
    "#     print(count,filename)\n",
    "    # dst = f\"{ids_list[file]}.pdf\"\n",
    "    # print(dst)\n",
    "    \n",
    "    # src =f\"{folder}/{filename}\"  # foldername/filename, if .py file is outside folder\n",
    "    # dst =f\"{folder}/{dst}\"\n",
    "         \n",
    "    # rename() function will\n",
    "    # rename all the files\n",
    "    # os.rename(src, dst)\n",
    "\n",
    "print(hash_list[30])\n",
    "\n",
    "# os.chdir(folder)\n",
    "# files = glob.glob(\"*.pdf\")\n",
    "# files.sort(key=os.path.getmtime)\n",
    "# for file in range(len(files)):\n",
    "#     print(file)\n",
    "    # filename = os.fsdecode(files[file])\n",
    "    # print(filename)\n",
    "    # # file_title = filename.split('\\\\')[1]\n",
    "    # dst = f\"{ids_list[file]}.pdf\"\n",
    "    # src =f\"{folder}/{filename}\"\n",
    "    # os.rename(src, dst)\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0037 1\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0064 2\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0008 3\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0029 4\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0041 5\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0034 6\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0063 7\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0015 8\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0044 9\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0009 10\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0016 11\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0012 12\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0039 13\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0036 14\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0006 15\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0067 16\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0027 17\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0010 18\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0020 19\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0025 20\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0007 21\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0026 22\n",
    "# https://courtforms.jud.state.ma.us/publicforms/PFC0075 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from selenium.webdriver.common.by import By\n",
    "def hashme(w):\n",
    "    h = hashlib.md5(w.encode('utf-8'))\n",
    "    return h.hexdigest()\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "import webbrowser\n",
    "import io\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 98.0.4758\n",
      "Get LATEST chromedriver version for 98.0.4758 google-chrome\n",
      "Driver [C:\\Users\\Owner\\.wdm\\drivers\\chromedriver\\win32\\98.0.4758.80\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://courtforms.jud.state.ma.us/publicforms/PFC0078 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_38424\\915889404.py:20: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_css_selector(\"[aria-label='Save as PDF'\").click()#this is me most common layout. However, not nearly the only one..\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dest_sample = 'PDFS/'\n",
    "df = pd.read_csv('All.csv')\n",
    "df_errors = df.loc[df['status']==1]\n",
    "url_list = list(df_errors['url'])\n",
    "ids_list = list(df_errors['id'])\n",
    "index_list = df_errors.index\n",
    "\n",
    "#print(index_list)\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "hash_list = []\n",
    "for url in range(76,len(url_list)):\n",
    "    time.sleep(2)\n",
    "    if 'www' not in url_list[url]:\n",
    "        print(url_list[url], url)\n",
    "        driver.get(url_list[url])\n",
    "        \n",
    "        driver.find_element_by_css_selector(\"[aria-label='Save as PDF'\").click()#this is me most common layout. However, not nearly the only one..\n",
    "        #as a result some where manually downloaded\n",
    "        # df.at[index_list[url],'status'] = 0\n",
    "\n",
    "#20, 31, 33, 37, 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = 'C:/Users/Owner/Downloads'\n",
    "df = pd.read_csv('All_cleaned2.csv')\n",
    "df_errors = df.loc[df['status']==1]\n",
    "url_list = list(df_errors['url'])\n",
    "ids_list = list(df_errors['id'])\n",
    "index_list = df_errors.index\n",
    "\n",
    "len(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder = 'C:/Users/Owner/Downloads'\n",
    "errors_set = set(list([20, 31, 33, 37, 41])) #these were unsuccesful attempts. 2 were docs and the other 4 needed to be filled out before downloading\n",
    "successful_ids = []\n",
    "\n",
    "for id in range(len(ids_list)):\n",
    "    if id not in errors_set:\n",
    "        successful_ids.append(ids_list[id])\n",
    "    # else:\n",
    "    #     print(ids_list[id])\n",
    "# for i in errors_set:\n",
    "#     print(ids_list[i])\n",
    "\n",
    "# navigate in our downloads directory and rename the files we just downloaded\n",
    "# os.chdir(folder)\n",
    "# files = glob.glob(\"*.pdf\") #but only the pdfs \n",
    "# files.sort(key=os.path.getmtime) # from the first (download) to the last so it is consistent with our database\n",
    "# for id in range(len(successful_ids)):\n",
    "   \n",
    "#     filename = os.fsdecode(files[id])\n",
    "#     print(filename)\n",
    "#     dst = f\"{successful_ids[id]}.pdf\" \n",
    "#     src =f\"{folder}/{filename}\"\n",
    "#     os.rename(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('All.csv')\n",
    "df_errors = df.loc[df['status']==1]\n",
    "url_list = list(df_errors['url'])\n",
    "ids_list = list(df_errors['id'])\n",
    "index_list = df_errors.index\n",
    "\n",
    "# to_change = df[df['id']==successful_ids[6]]\n",
    "\n",
    "for id in range(len(successful_ids)):\n",
    "    to_change = df[df['id']==successful_ids[id]].index\n",
    "    df.at[to_change,'status'] = 0\n",
    "df.to_csv(\"Trial.csv\",index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owner\\OneDrive\\Υπολογιστής\\LIT\n",
      "03ff8fc4e8e9168c47c61ce857f53ba1.pdf\n",
      "09421b53ae01890a5bfe5d48fb41f937.pdf\n",
      "09494b9c8f26fd1d3748900f525e768c.pdf\n",
      "09cd22913b272e4e60d59c14d388fbda.pdf\n",
      "10c5c46d0d0b1712442122fbdfa01980.pdf\n",
      "121fc167faa09d7a0c33831bb7b9dcdd.pdf\n",
      "1644fb48e9c2c6ebf2a35e9ed609ac1d.pdf\n",
      "17eb8758f02ccff1159a523a13229a15.pdf\n",
      "192751f62acf65c5c2166008c0cbd370.pdf\n",
      "1ee29baf037da4c60dc04ca70f2a4a1f.pdf\n",
      "232a8ad9e01a7f578b31582474608489.pdf\n",
      "2943546b42ecfb4f63f89f55e5fe6256.pdf\n",
      "2f6e28249ec0b137f4535412be58bde8.pdf\n",
      "2fcc5fc063b795d4eb374954954da9e6.pdf\n",
      "327a89994d1fedf2a88982046e83173c.pdf\n",
      "3392d95879515eeef21b797726e16e1e.pdf\n",
      "33d719afde61d46228c72914e3e04119.pdf\n",
      "3d0f229630dcc7a2805e5027b337ee4d.pdf\n",
      "42976a87d68aeb2b6b912dec0079e85b.pdf\n",
      "43b02307278ebafb6492ef928903646c.pdf\n",
      "471507fe406ac6bef098b0ba42dc4c17.pdf\n",
      "476ee0e5d057d94c05567a6ca7118cd7.pdf\n",
      "4d095ad65854ae90e0d0a8d81e89b42f.pdf\n",
      "557080ac38b381078e15493bbf409e2d.pdf\n",
      "5852a4de8c994bba975be406d08f8fed.pdf\n",
      "5ce8061f9ae91e3a58a07cfb92174ab6.pdf\n",
      "5eaa0c0eb360074f0f7b682b7a5cbd2e.pdf\n",
      "5fbd793d4d1673f1f8ade2924aee4080.pdf\n",
      "5fe5739ed1ea23ac18714bbb0eea518c.pdf\n",
      "660953d93c8879e708ac32fd6cb08294.pdf\n",
      "6709f5ff34e54dd02c28a1716a153c76.pdf\n",
      "6a9f26624bc51fdcd5830595b0b42b54.pdf\n",
      "6ab9c7a17b79f30fa383f6a79405c2a4.pdf\n",
      "73e0dede5b1f37052dc7030b65a7d09f.pdf\n",
      "7579284ed020c69ee7ade1002c23cd1f.pdf\n",
      "76837a7d3dd7dcda3d805e9b79b4852c.pdf\n",
      "8029883815d95026abb47f20a67885e3.pdf\n",
      "8262d4f76797e7849187b11ac5b03222.pdf\n",
      "82b3752304b9bcab663a9cf69e9837a7.pdf\n",
      "8614c68b198c49c55706f246644202d2.pdf\n",
      "90b807985877e6119bea8e0235036470.pdf\n",
      "92a4c502fd1303899dfa1cd090764e44.pdf\n",
      "92e9e10fedbebaffd700a5b40d8b0e3a.pdf\n",
      "9728d8e9604329c507e9df2bd92d7052.pdf\n",
      "998b897f4708d8260d1e6a20d53d4e03.pdf\n",
      "9edd275559dd099e043fd74894169842.pdf\n",
      "a22816268ad3958256a4d49c6f0af190.pdf\n",
      "a555dc61f87e8e89a83735783877d7d5.pdf\n",
      "a65a7244072cefea54b3024a7cedc24e.pdf\n",
      "a7c1f46638b2b7e244acef8d44550a4a.pdf\n",
      "a93750147c4e975b5e04becc83636eb4.pdf\n",
      "b0e7bc236861af4943e16fc25bf46b5c.pdf\n",
      "b5063e9ddc0c3808a3b31b89f0a69c18.pdf\n",
      "be0bfafdff869e6d2e696c92603df3c1.pdf\n",
      "be91a0a95c841c4db09aedb7db271446.pdf\n",
      "c280cb12c84f73535594fe14a90c15a4.pdf\n",
      "c417d255ae89d8abe3f0463ae21c466b.pdf\n",
      "c5f2c099b13184f6b8717b5e1f063bca.pdf\n",
      "c6d3c4e611ef18f8f8accf761de74664.pdf\n",
      "cdfcda95beda0f7d0156960b8a5c7b1a.pdf\n",
      "d3f55a24baf0c6defdd55e1e5a805fc9.pdf\n",
      "d3fa15bf4b9a428c4c446e05607d9555.pdf\n",
      "d4fe8ae4e9e6a9423aff6a8c62a7b9cc.pdf\n",
      "d61445b79542bb207b28f05cc6696911.pdf\n",
      "d97307b53e2220d308feee5662be82b2.pdf\n",
      "daa6a52516474c30f0418b4f09eb0b85.pdf\n",
      "ddb9f380154526f0bdf40f2d9578bee9.pdf\n",
      "de09665fabed604a1f64a401d028f525.pdf\n",
      "e542e8a2528de22f5c6725bba151cde4.pdf\n",
      "f5e3b1b2c6abe643455b9ecdcaa2f24c.pdf\n",
      "f798a7472e49b41a199151e722213417.pdf\n"
     ]
    }
   ],
   "source": [
    "retval = os.getcwd()\n",
    "print(retval)\n",
    "folder = 'PDFS/G'\n",
    "os.chdir(folder)\n",
    "files = glob.glob(\"*.pdf\")\n",
    "for file in files:\n",
    "    print(file)\n",
    "retval = os.getcwd()\n",
    "os.chdir(retval)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f68585656d2af418509cfdc3ae64a2df251c855d829aa9c64f1046c3e79900"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
