{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import bs4.builder._lxml\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import urllib.request, json \n",
    "import re\n",
    "import time\n",
    "from datetime import date\n",
    "from zipfile import ZipFile\n",
    "import hashlib\n",
    "import os\n",
    "from uuid import uuid1\n",
    "\n",
    "def hashme(w):\n",
    "    h = hashlib.md5(w.encode('utf-8'))\n",
    "    return h.hexdigest()\n",
    "   \n",
    "import requests\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.parse import quote\n",
    "\n",
    "#!pip install PyPDF2\n",
    "import PyPDF2\n",
    "\n",
    "from io import StringIO # https://stackoverflow.com/a/18284900\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "def convert(fname, pages=None):\n",
    "    if not pages:\n",
    "        pagenums = set()\n",
    "    else:\n",
    "        pagenums = set(pages)\n",
    "\n",
    "    output = StringIO()\n",
    "    manager = PDFResourceManager()\n",
    "    converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "    infile = open(fname, 'rb') # python three change\n",
    "    for page in PDFPage.get_pages(infile, pagenums, check_extractable=False):\n",
    "        interpreter.process_page(page)\n",
    "    infile.close()\n",
    "    converter.close()\n",
    "    text = output.getvalue()\n",
    "    output.close\n",
    "    return text\n",
    "   \n",
    "def download_pdf(url,path,filename):\n",
    "    try:\n",
    "       \n",
    "        filename = path+filename\n",
    "\n",
    "        f = requests.get(url, stream=True, headers={'User-agent': 'Mozilla/5.0'})\n",
    "\n",
    "        with open(filename, \"wb\") as code:\n",
    "            #code.write(f.read())\n",
    "            code.write(f.content)\n",
    "\n",
    "        return convert(filename)\n",
    "    except:\n",
    "        return \"error\"\n",
    "\n",
    "def connect_to_page(url):\n",
    "    connection = requests.get(url)\n",
    "    html = connection.text\n",
    "    soup = bs4.BeautifulSoup(html,\"lxml\")\n",
    "    connection.close()\n",
    "    return soup\n",
    "def download_pdf_unmasked(url,path,filename):\n",
    "   \n",
    "    filename = path+filename\n",
    "\n",
    "    f = requests.get(url, stream=True, headers={'User-agent': 'Mozilla/5.0'})\n",
    "\n",
    "    with open(filename, \"wb\") as code:\n",
    "        #code.write(f.read())\n",
    "        code.write(f.content)\n",
    "\n",
    "    return convert(filename)\n",
    "\n",
    "   \n",
    "def download_docx(book_link, book_name):\n",
    "    the_book = requests.get(book_link, stream=True)\n",
    "    with open(book_name, 'wb') as f:\n",
    "      for chunk in the_book.iter_content(1024 * 1024 * 2):  # 2 MB chunks\n",
    "        f.write(chunk)\n",
    "\n",
    "def download_zip(url):\n",
    "    # Downloading the file by sending the request to the URL\n",
    "    req = requests.get(url)\n",
    "    # Split URL to get the file name\n",
    "    filename = url.split('/')[-1]\n",
    "    # Writing the file to the local file system\n",
    "    with open(filename,'wb') as output_file:\n",
    "        output_file.write(req.content)\n",
    "    print('Downloading Completed')\n",
    "\n",
    "    \n",
    "def extract_zip (name): \n",
    "    # specifying the zip file name  \n",
    "    file_name = name    \n",
    "    try:\n",
    "    # opening the zip file in READ mode\n",
    "        with ZipFile(file_name, 'r') as zip:\n",
    "            # printing all the contents of the zip file\n",
    "            zip.printdir()\n",
    "        \n",
    "            # extracting all the files\n",
    "            print('Extracting all the files now...')\n",
    "            zip.extractall()\n",
    "            print('Done!')\n",
    "    except:\n",
    "        return \"error\"\n",
    "def scan_dir(dir_name):\n",
    "    if dir_name == \"Transfer – Forum NC Orders\":\n",
    "        dir_name = \"Transfer - Forum NC Orders\"\n",
    "    out_list = []\n",
    "    print(\"---------------------{}----------------------\".format(dir_name))\n",
    "    for filename in os.scandir(dir_name):\n",
    "        if filename.is_file():\n",
    "            # print(filename.path.split(\"/\")[-1].split(\".\")[0])\n",
    "            title = filename.path.split(\"\\\\\")[-1]\n",
    "            file_name = title\n",
    "            out_list.append(title)\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://georgiacourts.gov/statecourt/state-court-behind-the-bench/state-court-forms/#1566231380414-5e189c6c-bce0', 'https://georgiacourts.gov/statecourt/state-court-behind-the-bench/state-court-forms/#1566244668513-d643f872-93bf', 'https://georgiacourts.gov/statecourt/state-court-behind-the-bench/state-court-forms/#1566244969529-97e234f4-7038', 'https://georgiacourts.gov/statecourt/state-court-behind-the-bench/state-court-forms/#1566241462728-b588212e-e7e2', 'https://georgiacourts.gov/statecourt/state-court-behind-the-bench/state-court-forms/#1566242970836-b2a5117a-f6bb']\n",
      "['Civil Forms', 'Juror Issues', 'Recusal Orders', 'Criminal Forms', 'Other Criminal Items']\n"
     ]
    }
   ],
   "source": [
    "BASE = \"https://georgiacourts.gov\"\n",
    "start = \"https://georgiacourts.gov/statecourt/state-court-behind-the-bench/state-court-forms/\"\n",
    "\n",
    "\n",
    "soup = connect_to_page(start)\n",
    "cat_refs = []\n",
    "cat_names = []\n",
    "\n",
    "\n",
    "for i in soup.find_all(\"h4\", class_ = \"vc_tta-panel-title vc_tta-controls-icon-position-left\"):\n",
    "    a = i.find(\"a\", href = True)\n",
    "    cat_refs.append(start+a['href'])\n",
    "    cat_names.append(a.text)\n",
    "print(cat_refs)\n",
    "print(cat_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today().strftime(\"%Y-%m-%d\")\n",
    "files_df = pd.DataFrame([],columns=[\"id\",\"jurisdiction\",\"source\",\"title\",\"group\",\"url\",\"filename\",\"downloaded\",\"type\"])\n",
    "form_dest = \"All_forms/\"\n",
    "jur = 'GA'\n",
    "\n",
    "\n",
    "soup = connect_to_page(cat_refs[0])\n",
    "wrapper = soup.find_all(\"div\", class_ = \"vc_tta-panel-body\")\n",
    "group_num = 0\n",
    "\n",
    "for cat in wrapper:\n",
    "    group_outer = cat_names[group_num]\n",
    "    group_num+=1\n",
    "    print(\"-------------{}---------------------\".format(group_outer))\n",
    "    links_per_cat =  cat.find_all(\"a\", href=True)\n",
    "\n",
    "    for ref in links_per_cat:\n",
    "        if \".zip\" in ref['href']:\n",
    "            url = BASE+ref['href']\n",
    "            zip_name = url.split(\"/\")[-1]\n",
    "            folder_name = re.sub(r\"\\([^()]*\\)\",\"\",ref.text)\n",
    "            folder_name = folder_name.strip()\n",
    "            # print(name)\n",
    "            zip = download_zip(url)\n",
    "            print(zip_name)\n",
    "            status = extract_zip(zip_name)\n",
    "            fileid = hashme(url)\n",
    "            group_inner = zip_name.split(\".\")[0]\n",
    "            if status!=\"error\":\n",
    "                # print(\"Transfer - Forum NC Orders\" == \"Transfer – Forum NC Orders\")\n",
    "                files_in_zip = scan_dir(folder_name)\n",
    "                for file in files_in_zip:\n",
    "                    print(file, \" IN FOR LOOP\")\n",
    "                    if \"doc\" in file.split(\".\")[1]:\n",
    "                        \n",
    "                        file_type =  \"doc\"\n",
    "                    else:\n",
    "                        file_type = \"pdf\"\n",
    "                    # fileid = uuid1().hex\n",
    "                    files_df = files_df.append(pd.DataFrame([[fileid,jur,start,file,group_inner,url,file,today, file_type]],columns=[\"id\",\"jurisdiction\",\"source\",\"title\",\"group\",\"url\",\"filename\",\"downloaded\", \"type\"]))\n",
    "        \n",
    "            else:\n",
    "                print(\"---------------\")\n",
    "                print(\"ERROR: \", zip_name , group_inner)\n",
    "                print(\"---------------\")\n",
    "\n",
    "            \n",
    "        elif \".doc\" in ref['href'] and \"http\" not in ref['href']:\n",
    "            url = BASE + ref['href']\n",
    "            fileid = hashme(url)\n",
    "            filename = url.split(\"/\")[-1].split(\".\")[0]\n",
    "            file_type = \"doc\"\n",
    "            title = ref.text\n",
    "            download_docx(url, fileid+'.{}'.format(file_type))\n",
    "            files_df = files_df.append(pd.DataFrame([[fileid,jur,start,title,group_outer,url,filename,today, file_type]],columns=[\"id\",\"jurisdiction\",\"source\",\"title\",\"group\",\"url\",\"filename\",\"downloaded\", \"type\"]))\n",
    "\n",
    "            print(url)\n",
    "\n",
    "        elif \".doc\" in ref['href']: #case where \"http\" part of string\n",
    "            url = ref['href']\n",
    "            fileid = hashme(url)\n",
    "            filename = url.split(\"/\")[-1].split(\".\")[0]\n",
    "            file_type = \"doc\"\n",
    "            title = ref.text\n",
    "            download_docx(url, fileid+'.{}'.format(file_type))\n",
    "            files_df = files_df.append(pd.DataFrame([[fileid,jur,start,title,group_outer,url,filename,today, file_type]],columns=[\"id\",\"jurisdiction\",\"source\",\"title\",\"group\",\"url\",\"filename\",\"downloaded\", \"type\"]))\n",
    "            print(ref.text)\n",
    "            print(url)\n",
    "\n",
    "        else: #case where its .pdf\n",
    "            url = BASE + ref['href']\n",
    "            fileid = hashme(url)\n",
    "            filename = url.split(\"/\")[-1].split(\".\")[0]\n",
    "            file_type = \"pdf\"\n",
    "            title = ref.text\n",
    "            download_pdf(url, form_dest,fileid+'.{}'.format(file_type))\n",
    "            files_df = files_df.append(pd.DataFrame([[fileid,jur,start,title,group_outer,url,filename,today, file_type]],columns=[\"id\",\"jurisdiction\",\"source\",\"title\",\"group\",\"url\",\"filename\",\"downloaded\", \"type\"]))\n",
    "\n",
    "            print(ref.text)\n",
    "            print(url)\n",
    "\n",
    "files_df.to_csv(\"GA-V1.csv\",index=False, encoding=\"utf-8\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GA was the first state I encountered with compressed folders containing multiple forms (i.e. zip files). I downloaded them and extracted their content. They were assigned a unique id, which was shared among all the forms inside the zip file. Another issue with files inside the zip was that a filename could not be extracted (title is the same as the filename). \n",
    "\n",
    "1. Extracts the different categories and the links to those categories\n",
    "2. Extracts the individual forms and zip files from the categories. Due to the presence of zip files, some additional functions are used (look helper functions)\n",
    "\n",
    "Helper functions are essential functions used in every states code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38e93288a63f880ac43b31dc367b31574b4ba5072d18091ce34e4d9aa2e581f6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
